{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf615d-08ad-424b-88ad-15f6a9dc9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import neural_policies\n",
    "import main\n",
    "import game_utils\n",
    "import yaml\n",
    "import torch\n",
    "import policy_gradient\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "\n",
    "seed = 1\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game = game_utils.create_random_normal_game(5)\n",
    "game = pyspiel.load_game(\"leduc_poker\")\n",
    "nn_config_file = \"../config/mlp.yaml\"\n",
    "reinforce_config_file = \"../config/reinforce_train.yaml\"\n",
    "nn_config = yaml.safe_load(open(nn_config_file, \"r\"))\n",
    "reinforce_config = yaml.safe_load(open(reinforce_config_file, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecd572",
   "metadata": {},
   "outputs": [],
   "source": [
    "nash_policy, nash_value = main.nash_equilibrium_policy_and_value(game)\n",
    "print(\"nash value\", nash_value)\n",
    "nn_player = 0\n",
    "nn_policy = neural_policies.create_policy_net(game, nn_config)\n",
    "nn_tab_policy = neural_policies.nn_to_tabular_policy(game, nn_policy, nn_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77136e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernet = main.train_hypernet_actionoutput(game, nn_config, nn_player, nn_policy, \"../config/hypernet.yaml\", \"../config/hypernet_reinforce_train.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5aee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_policy = neural_policies.create_policy_net(game, nn_config)\n",
    "nn_tab_policy = neural_policies.nn_to_tabular_policy(game, nn_policy, nn_player)\n",
    "main.tabular_br(game, nn_player, nn_tab_policy)\n",
    "main.policy_gradient_br(\n",
    "    game, nn_config, reinforce_config, nn_player, nn_policy, nn_tab_policy\n",
    ")\n",
    "main.hypernet_br(game, hypernet, nn_player, nn_policy, nn_tab_policy)\n",
    "\n",
    "lst_policy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c42880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(game.row_utilities())\n",
    "\n",
    "new_input_nn = neural_policies.create_policy_net(game, nn_config)\n",
    "input_probs = neural_policies.nn_to_tabular_policy(game, new_input_nn, nn_player).action_probability_array\n",
    "# input_probs = neural_policies.get_nn_probs(new_input_nn, game.new_initial_state(), nn_player)\n",
    "# print(\"input probs\", input_probs)\n",
    "\n",
    "playable_states = game_utils.all_game_states(game, 1-nn_player)\n",
    "state = playable_states[0]\n",
    "\n",
    "# print(neural_policies.get_hypernet_probs(hypernet, new_input_nn, state, 1-nn_player))\n",
    "# print(hypernet(new_input_nn, torch.tensor(state.information_state_tensor(1-nn_player))))\n",
    "nn_response = neural_policies.nn_to_tabular_policy(game, hypernet, 1-nn_player, new_input_nn)\n",
    "# print(\"nn response\", nn_response.action_probability_array)\n",
    "br = main.compute_best_response_tabular_policy(game, nn_tab_policy, 1 - nn_player)\n",
    "print(lst_policy)\n",
    "if lst_policy is not None:\n",
    "    cur_policy = br.action_probability_array\n",
    "    # find rows that differ\n",
    "    diff = np.where(cur_policy != lst_policy)\n",
    "    print(\"diff\", diff)\n",
    "lst_policy = br.action_probability_array\n",
    "print(br.action_probability_array)\n",
    "\n",
    "# neural_br_policy = neural_policies.create_policy_net(game, nn_config)\n",
    "# trainer = policy_gradient.PolicyGradientTrainer(\n",
    "#     game, neural_br_policy, train_params=reinforce_config\n",
    "# )\n",
    "# trainer.train_best_response(nn_policy, 1-nn_player)\n",
    "# print(neural_br_policy(torch.tensor(game.new_initial_state().information_state_tensor(1-nn_player))))\n",
    "# print(neural_policies.get_nn_probs(neural_br_policy, game.new_initial_state(), 1-nn_player))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404346e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# pprint(list(hypernet._model.get_weights()))\n",
    "mm = hypernet._model\n",
    "[l.bias for l in mm.layers]\n",
    "# mm.layers[-1].weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
