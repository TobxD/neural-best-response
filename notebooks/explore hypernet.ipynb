{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf615d-08ad-424b-88ad-15f6a9dc9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import importlib\n",
    "import neural_policies\n",
    "import main\n",
    "import game_utils\n",
    "import policy_gradient\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pyspiel\n",
    "from types import SimpleNamespace\n",
    "\n",
    "importlib.reload(main)\n",
    "importlib.reload(neural_policies)\n",
    "importlib.reload(game_utils)\n",
    "importlib.reload(policy_gradient)\n",
    "\n",
    "seed = 1\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game = game_utils.create_random_normal_game(5)\n",
    "# game = main.create_game(SimpleNamespace(game=\"rps\"))\n",
    "game = main.create_game(SimpleNamespace(game=\"kuhn_poker\"))\n",
    "# nn_config_file = \"../config/linear_mlp.yaml\"\n",
    "nn_config_file = \"../config/kuhn_mlp.yaml\"\n",
    "# nn_config_file = \"../config/kuhn_big.yaml\"\n",
    "reinforce_config_file = \"../config/reinforce_train.yaml\"\n",
    "nn_config = yaml.safe_load(open(nn_config_file, \"r\"))\n",
    "reinforce_config = yaml.safe_load(open(reinforce_config_file, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecd572",
   "metadata": {},
   "outputs": [],
   "source": [
    "nash_policy, nash_value = main.nash_equilibrium_policy_and_value(game)\n",
    "print(\"nash value\", nash_value)\n",
    "nn_player = 0\n",
    "nn_policy = neural_policies.create_policy_net(game, nn_config)\n",
    "nn_tab_policy = neural_policies.nn_to_tabular_policy(game, nn_policy, nn_player)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ca92c",
   "metadata": {},
   "source": [
    "# ignore from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = game_utils.all_game_states(game, 0)[0]\n",
    "# s.legal_actions_mask()\n",
    "# s.information_state_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77136e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_gradient import PolicyGradientHypernetTrainer\n",
    "\n",
    "# hypernet = main.train_hypernet_actionoutput(game, nn_config, nn_player, nn_policy, \"../config/hypernet.yaml\", \"../config/hypernet_reinforce_train.yaml\")\n",
    "\n",
    "hypernet_config = \"../config/hypernet.yaml\"\n",
    "hypernet_config = yaml.safe_load(open(hypernet_config, \"r\"))\n",
    "hypernet_train_config = \"../config/hypernet_reinforce_train.yaml\"\n",
    "hypernet_train_config = yaml.safe_load(open(hypernet_train_config, \"r\"))\n",
    "\n",
    "mlp = neural_policies.create_policy_net(game, nn_config)\n",
    "# hypernet = neural_policies.create_hypernet_nn_output(game, mlp, hypernet_config)\n",
    "hypernet = neural_policies.create_hypernet_actionoutput(game, mlp, hypernet_config)\n",
    "trainer = PolicyGradientHypernetTrainer(game, hypernet, hypernet_train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_best_response(nn_config, 1 - nn_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5aee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_policy = neural_policies.create_policy_net(game, nn_config)\n",
    "nn_tab_policy = neural_policies.nn_to_tabular_policy(game, nn_policy, nn_player)\n",
    "main.tabular_br(game, nn_player, nn_tab_policy)\n",
    "main.policy_gradient_br(\n",
    "    game, nn_config, reinforce_config, nn_player, nn_policy, nn_tab_policy\n",
    ")\n",
    "main.hypernet_br(game, hypernet, nn_player, nn_policy, nn_tab_policy)\n",
    "\n",
    "lst_policy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c42880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(game.row_utilities())\n",
    "\n",
    "new_input_nn = neural_policies.create_policy_net(game, nn_config)\n",
    "input_probs = neural_policies.nn_to_tabular_policy(game, new_input_nn, nn_player).action_probability_array\n",
    "# input_probs = neural_policies.get_nn_probs(new_input_nn, game.new_initial_state(), nn_player)\n",
    "# print(\"input probs\", input_probs)\n",
    "\n",
    "playable_states = game_utils.all_game_states(game, 1-nn_player)\n",
    "state = playable_states[0]\n",
    "\n",
    "# print(neural_policies.get_hypernet_probs(hypernet, new_input_nn, state, 1-nn_player))\n",
    "# print(hypernet(new_input_nn, torch.tensor(state.information_state_tensor(1-nn_player))))\n",
    "nn_response = neural_policies.nn_to_tabular_policy(game, hypernet, 1-nn_player, new_input_nn)\n",
    "# print(\"nn response\", nn_response.action_probability_array)\n",
    "br = main.compute_best_response_tabular_policy(game, nn_tab_policy, 1 - nn_player)\n",
    "print(lst_policy)\n",
    "if lst_policy is not None:\n",
    "    cur_policy = br.action_probability_array\n",
    "    # find rows that differ\n",
    "    diff = np.where(cur_policy != lst_policy)\n",
    "    print(\"diff\", diff)\n",
    "lst_policy = br.action_probability_array\n",
    "print(br.action_probability_array)\n",
    "\n",
    "# neural_br_policy = neural_policies.create_policy_net(game, nn_config)\n",
    "# trainer = policy_gradient.PolicyGradientTrainer(\n",
    "#     game, neural_br_policy, train_params=reinforce_config\n",
    "# )\n",
    "# trainer.train_best_response(nn_policy, 1-nn_player)\n",
    "# print(neural_br_policy(torch.tensor(game.new_initial_state().information_state_tensor(1-nn_player))))\n",
    "# print(neural_policies.get_nn_probs(neural_br_policy, game.new_initial_state(), 1-nn_player))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404346e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# pprint(list(hypernet._model.get_weights()))\n",
    "mm = hypernet._model\n",
    "[l.bias for l in mm.layers]\n",
    "# mm.layers[-1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3d109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385aa77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import input_net_data\n",
    "importlib.reload(input_net_data)\n",
    "# input_nets = input_net_data.read_all_nn(game, \"../nn_inputs/kuhn\")\n",
    "input_nets = input_net_data.read_all_nn(game, \"../nn_inputs/kuhn-0-big\")\n",
    "# input_nets = input_net_data.read_all_nn(game, \"../nn_inputs/rps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = -1\n",
    "nn_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf9dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_policy = neural_policies.create_policy_net(game, nn_config)\n",
    "ind = ind + 1\n",
    "# import random\n",
    "# ind = random.choice([i for i in range(5)])\n",
    "ind = ind % 5\n",
    "print(\"ind\", ind)\n",
    "# print(input_nets[ind].get_weights())\n",
    "nn_tab_policy = neural_policies.nn_to_tabular_policy(game, input_nets[ind], 0)\n",
    "# br_pol = main.tabular_br(game, nn_player, nn_tab_policy)\n",
    "print(nn_tab_policy.action_probability_array)\n",
    "\n",
    "# print(nn_tab_policy.action_probability_array)\n",
    "\n",
    "br = main.compute_best_response_tabular_policy(game, nn_tab_policy, 1)\n",
    "print(br.action_probability_array)\n",
    "combined_policy = main.combine_tabular_policies(game, nn_tab_policy, br)\n",
    "print(\"value of table br policy\", main.policy_value(game, combined_policy))\n",
    "\n",
    "nn_br = neural_policies.nn_to_tabular_policy(game, hypernet, 1-nn_player, input_nets[ind])\n",
    "print(nn_br.action_probability_array)\n",
    "diff = np.abs(br.action_probability_array - nn_br.action_probability_array).sum()\n",
    "print(diff)\n",
    "combined_policy = main.combine_tabular_policies(game, nn_tab_policy, nn_br)\n",
    "print(\"value of table br policy\", main.policy_value(game, combined_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbacf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in combined_policy.states:\n",
    "    print(s.information_state_tensor(), s.information_state_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1e5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e05cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nfn.common import state_dict_to_tensors\n",
    "state_dict_to_tensors(input_nets[0].state_dict())[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = input_nets[0]\n",
    "w1 = list(nn.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7adb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca9f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4675cb12",
   "metadata": {},
   "source": [
    "# relevant from here, next two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import policy_gradient\n",
    "importlib.reload(policy_gradient)\n",
    "importlib.reload(neural_policies)\n",
    "\n",
    "hypernet_config = \"../config/hypernet.yaml\"\n",
    "hypernet_config = yaml.safe_load(open(hypernet_config, \"r\"))\n",
    "hypernet_train_config = \"../config/hypernet_reinforce_train.yaml\"\n",
    "hypernet_train_config = yaml.safe_load(open(hypernet_train_config, \"r\"))\n",
    "\n",
    "mlp = neural_policies.create_policy_net(game, nn_config)\n",
    "hypernet_nn_output = neural_policies.create_hypernet_nn_output(game, mlp, hypernet_config)\n",
    "trainer = policy_gradient.PolicyGradientHypernetTrainer(game, hypernet_nn_output, hypernet_train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca17333",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_nets = trainer.train_simultaneous_br(nn_config)\n",
    "# trainer.train_best_response(policy_nets, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf78ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = -1\n",
    "nn_config\n",
    "game\n",
    "input_nets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_policy = neural_policies.create_policy_net(game, nn_config)\n",
    "ind = ind + 1\n",
    "nn_player = 1\n",
    "nn_policy = input_nets[ind]\n",
    "nn_tab_policy = neural_policies.nn_to_tabular_policy(game, nn_policy, nn_player)\n",
    "# br_pol = main.tabular_br(game, 1-nn_player, nn_tab_policy)\n",
    "print(nn_tab_policy.action_probability_array)\n",
    "\n",
    "# main.policy_gradient_br(\n",
    "#     game, nn_config, reinforce_config, nn_player, nn_policy, nn_tab_policy\n",
    "# )\n",
    "\n",
    "br = main.compute_best_response_tabular_policy(game, nn_tab_policy, 1-nn_player)\n",
    "print(br.action_probability_array)\n",
    "policies = [nn_tab_policy, br] if nn_player == 0 else [br, nn_tab_policy]\n",
    "combined_policy = main.combine_tabular_policies(game, *policies)\n",
    "print(\"value of table br policy\", main.policy_value(game, combined_policy))\n",
    "\n",
    "nn_br = neural_policies.nn_to_tabular_policy(game, policy_nets[1-nn_player], 1-nn_player, nn_policy)\n",
    "print(nn_br.action_probability_array)\n",
    "policies = [nn_tab_policy, nn_br] if nn_player == 0 else [nn_br, nn_tab_policy]\n",
    "combined_policy = main.combine_tabular_policies(game, *policies)\n",
    "print(\"value of hypernet br policy\", main.policy_value(game, combined_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61530746",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnout = policy_nets[1].model_output(input_nets[ind])\n",
    "[print(x) for x in pnout.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df964a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnout\n",
    "game.information_state_tensor_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = pyspiel.load_game(\"leduc_poker\")\n",
    "g2.information_state_tensor_shape()\n",
    "s = game.new_initial_state()\n",
    "s.information_state_tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88051cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3df514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ee421d7",
   "metadata": {},
   "source": [
    "# test setting weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_policies\n",
    "importlib.reload(neural_policies)\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed76a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = neural_policies.create_policy_net(game, nn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ac4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = torch.ones_like(mlp.get_weights(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b838d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights.requires_grad, new_weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ee988",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cd11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.network[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mlp.forward(torch.zeros(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = res.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b041f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.network[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf42995",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e83cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82efd974",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(2, 2)\n",
    "linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9323c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = torch.ones(8, requires_grad=True)\n",
    "# linear.weight = nn.Parameter(new_weights)\n",
    "linear.weight.requires_grad = False\n",
    "linear.weight.copy_(new_weights[:4].view(linear.weight.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.ones(2, requires_grad=True)\n",
    "loss = linear(inp).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e14a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.grad\n",
    "linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ae9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
